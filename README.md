# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
The classification goal of this project is to predict if the client will subscribe (yes/no) a term deposit (variable y).  The data used for the project was collected from 
direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.


The project was done both with a hyperparameter run and a autoML run with the objective to compare the results using both methods for accuracy.  The most accurate method was VotingEnsemble with an accuracy of .9174.  This was model was obtained as a result of the AutomML run. 


## Scikit-learn Pipeline
The major steps for preparing the pipeline are common for all machine learning.  Import, Clean, Split Data and select a model 
<ol>
  <li>Import and Clean and Split the Data
    <ul>
      <li> Import data using <i>TabularDatasetFactory</i> </li>
      <li> Cleaning of data -  handling NULL values, processing dates </li>
      <li> Splitting the cleaned data into train and test data </li>
      <li> For this part of the experiment we selected scikit-learn logistic regression model for classification </li> 
    </ul>
  </li><br>
  <li>Setup the estimator and pass that data to the Hyperdrive</li><br>
  <li> Configuration of Hyperdrive 
    <ul>
      <li> Setup of parameter sampler </li>
      <li> Setup of primary metric </li>
      <li> Setup of early termination policy </li>
      <li> Setup of estimator (SKLearn) </li>
      <li> Setup resources </li>
   </ul>
  </li><br>  
  <li>Save the trained optimized model</li>
</ol>

<p>As specified in the project requirements, we have used logistic regression model for our binary classification problem and Hyperdrive tool to choose the best hyperparameter values from the range of values provided. 
The best Model from the Hyperdrive run is listed below. </p> 

<strong>Best Hyperdrive Model</strong>
<i>ID :  HD_5355379d-5443-42ae-9375-61cf4708bf81_1
Metrics :  {'Regularization Strength:': 10.0, 'Max iterations:': 50, 'Accuracy': 0.914921598381386} </i>

<img src = "https://github.com/slcdlvpr/PipelineSetup/blob/main/Images/Cleanup.JPG" />


<strong>Parameter Sampling</strong>
I used RandomParameterSampling because it supports both discrete and continuous hyperparameters. It supports early termination of low-performance runs and supports early stopping policies.  In random sampling, hyperparameter values are randomly selected from the defined search space. This sampling technique was shown to be comparable to full grid sampling results in earlier labs.  

<strong>Early Stopping Policy</strong>
<p> The early stopping policy I chose was Bandit Policy because it is based on slack factor and evaluation interval. Bandit terminates runs where the primary metric is not within the specified slack factor compared to the best performing run. <a href = 'https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.banditpolicy?view=azure-ml-py&preserve-view=true#&preserve-view=truedefinition'> More Info</a></p



## AutoML
AutoML setup process was similar to setup for the hyperparameter run. 
<ol>
  <li> Import data using <i>TabularDatasetFactory</i></li>
  <li> Cleaning of data -  handling NULL values etc. </li>
  <li> Splitting of data into train and test data </li>
  <li> Configuration of AutoML parameters </li>
  <li> Save the best model generated </li>
</ol>

**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
The two methods produced very similar results with a slight edge in accuracy going to AutoML.  The AutoML run took longer and used a wider range of potential models to test for the most accurate.  The steps for preparing each run were similar as well. 

## Future work
One of the DATA GUARDRAILS triggered during the AutoML run.  The nature of the trigger was -- Imbalanced data can lead to a falsely perceived positive effect of a model's accuracy -- so there is probably some tweaks that could be made to the split on the training/testing data. 


## Proof of cluster clean up
<img src = "https://github.com/slcdlvpr/PipelineSetup/blob/main/Images/Cleanup.JPG"/>
